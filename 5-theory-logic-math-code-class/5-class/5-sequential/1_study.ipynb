{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "\n",
    "Creating a neural network from scratch involves understanding the fundamental components and their interactions.\n",
    "\n",
    "We shall study here designing sequential model using \n",
    "- `Neuron`\n",
    "- `Layer`\n",
    "- `Sequential`\n",
    "\n",
    "we shall also use:\n",
    "- `Sigmoid` (activation)\n",
    "- `MeanSquaredError`(loss)\n",
    "- `SGD` (Optimizer)\n",
    "\n",
    "<br/>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Neuron` Class\n",
    "\n",
    "we already developed `Neuron` class in [Neuron.py](../../lib/current/src/neuron/Neuron.py).\n",
    "\n",
    "but to understand easily, we shall use a simpler version here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "  output = None\n",
    "  input = None\n",
    "  delta = None\n",
    "\n",
    "  def __init__(self, num_inputs, activation):\n",
    "    self.weights = np.repeat(0.5, num_inputs)\n",
    "    self.bias = 0.2\n",
    "    self.activation = activation\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.input = inputs\n",
    "    z = np.dot(inputs, self.weights) + self.bias\n",
    "    self.output = self.activation.apply(z)\n",
    "    return self.output\n",
    "\n",
    "  def backward(self, error, learning_rate):\n",
    "    derivative = self.activation.derivative(self.output)\n",
    "    self.delta = error * derivative\n",
    "    self.weights -= learning_rate * self.delta * self.input\n",
    "    self.bias -= learning_rate * self.delta\n",
    "    return np.dot(self.delta, self.weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our `Neuron` need an `activation` object in consttructor for initialization.\n",
    "\n",
    "in `forward`, it applies activation on weighted sum\n",
    "\n",
    "in `backward` it applies SGD, and returns error gradient for previous neurons\n",
    "\n",
    "<br/>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Layer` Class\n",
    "\n",
    "`Layer` of `Neuron`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  def __init__(self, num_neurons, num_inputs, activation):\n",
    "    self.neurons = [Neuron(num_inputs, activation) for _ in range(num_neurons)]\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    outputs = np.array([neuron.forward(inputs) for neuron in self.neurons])\n",
    "    return outputs\n",
    "\n",
    "  def backward(self, errors, learning_rate):\n",
    "    next_errors = np.zeros(len(self.neurons[0].weights))\n",
    "    for i, neuron in enumerate(self.neurons):\n",
    "      next_errors += neuron.backward(errors[i], learning_rate)\n",
    "    return next_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__init__` (constructor) initialize all `Neuron`s\n",
    "\n",
    "`forward` predicts outputs of all neurons\n",
    "\n",
    "`backward` performs backward for every `Neuron` and calculate sum for previous layers\n",
    "\n",
    "<br/>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Sequential` Class\n",
    "\n",
    "make an ANN from `Layer`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "  def __init__(self, layers=None):\n",
    "    self.layers = layers or []\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    for layer in self.layers:\n",
    "      inputs = layer.forward(inputs)\n",
    "    return inputs\n",
    "\n",
    "  def backward(self, error, learning_rate):\n",
    "    for layer in reversed(self.layers):\n",
    "      error = layer.backward(error, learning_rate)\n",
    "\n",
    "  def train(self, X, y, loss, epochs, learning_rate):\n",
    "    for _ in range(epochs):\n",
    "      for i in range(len(X)):\n",
    "        output = self.forward(X[i])\n",
    "        error = loss.calculate(y[i], output)\n",
    "        self.backward(error, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__init__` initializes layers. `add` appends\n",
    "\n",
    "`forward` iteratively pass inputs of previous layer to next layer and returns output of last layer\n",
    "\n",
    "`backward` performs reverse iteration on layers and pass gradient to previous layers\n",
    "\n",
    "`train` apply epochs, for every epoch, output is chained from input layer to last layer, then output of last layer is fed to loss function. once error is calculated, loss is chained back for GD\n",
    "\n",
    "<br/>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation and Loss classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "  def apply(self, x): return 1 / (1 + np.exp(-x))\n",
    "  def derivative(self, x): return x * (1 - x)\n",
    "\n",
    "class Loss:\n",
    "  def calculate(self, y, y_pred): return y_pred - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Layer(3, 3, Activation()),\n",
    "  Layer(1, 3, Activation()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "  X, Y,\n",
    "  loss=Loss(),\n",
    "  epochs=20000,\n",
    "  learning_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted = 1.0 actual = 1\n",
      "predicted = 0.0 actual = 0\n",
      "predicted = 1.0 actual = 1\n",
      "predicted = 0.0 actual = 0\n",
      "predicted = 0.0 actual = 0\n",
      "predicted = 0.0 actual = 0\n",
      "predicted = 1.0 actual = 1\n",
      "predicted = 1.0 actual = 1\n",
      "predicted = 1.0 actual = 1\n",
      "predicted = 0.0 actual = 0\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "for i in range(len(X)):\n",
    "  print('predicted =', round(model.forward(X[i])[0], 1), 'actual =', Y[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
