{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perameters\n",
    "\n",
    "- `learning_rate`\n",
    "- `activation`\n",
    "- `loss`\n",
    "- `optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Imports\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "from numbers import Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Customer Exceptions\n",
    "#\n",
    "\n",
    "class InvalidInitialWeights(Exception): pass\n",
    "\n",
    "class UnexpectedInputsShape(Exception): pass\n",
    "class UnexpectedOutputsShape(Exception): pass\n",
    "\n",
    "class InvalidActivation(Exception): pass\n",
    "class InvalidOptimizer(Exception): pass\n",
    "class InvalidLoss(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Sigmoid Activation\n",
    "#\n",
    "\n",
    "class Sigmoid():\n",
    "\n",
    "  def apply(self, x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "  \n",
    "  def derivative(self, x):\n",
    "    sig = self.apply(x)\n",
    "    return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Mean Squared Error\n",
    "#\n",
    "\n",
    "class MeanSquaredError():\n",
    "  N = 0\n",
    "\n",
    "  def calculate(self, y_true:Iterable, y_pred:Iterable):\n",
    "    self.N = len(y_true)\n",
    "    return np.mean(np.square(y_true - y_pred))\n",
    "  \n",
    "  def derivative(self, y_true:Number, y_pred:Number):\n",
    "    return -2*(y_true-y_pred)/self.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Stochastic Gradient Descent\n",
    "#\n",
    "\n",
    "class StochasticGradientDescent():\n",
    "  MODES = {\n",
    "    'SAMPLE': 1,\n",
    "    'BATCH': 2,\n",
    "  }\n",
    "\n",
    "  def __init__(self, learning_rate=None):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.mode = self.MODES['SAMPLE']\n",
    "\n",
    "  def update(self, model, learning_rate=0.01):\n",
    "    # learning rate\n",
    "    lr = self.learning_rate or model.learning_rate or learning_rate\n",
    "\n",
    "    # hierarchy functions\n",
    "    def update_neuron(neuron, lr): neuron.update(lr)\n",
    "    def update_layer(layer, lr):\n",
    "      for neuron in layer.neurons: update_neuron(neuron, lr)\n",
    "\n",
    "    # if ann\n",
    "    if hasattr(model, 'layers'):\n",
    "      for layer in model.layers: update_layer(layer, lr)\n",
    "    \n",
    "    # if layer\n",
    "    elif hasattr(model, 'neurons'):\n",
    "      for neuron in model.neurons: update_neuron(neuron, lr)\n",
    "    \n",
    "    # if neuron\n",
    "    elif hasattr(model, 'forward'):\n",
    "      update_neuron(model, lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "\n",
    "  # \n",
    "  # Props\n",
    "  # \n",
    "\n",
    "  builtin:dict = {\n",
    "    'activation': {\n",
    "      'default': Sigmoid,\n",
    "      'sigmoid': Sigmoid,\n",
    "    },\n",
    "    'loss_function': {\n",
    "      'default': MeanSquaredError,\n",
    "      'sigmoid': MeanSquaredError,\n",
    "    },\n",
    "    'optimizer': {\n",
    "      'default': StochasticGradientDescent,\n",
    "      'sgd': StochasticGradientDescent,\n",
    "    }\n",
    "  }\n",
    "\n",
    "  errors:dict = {\n",
    "    'activation':    InvalidActivation,\n",
    "    'loss_function': InvalidLoss,\n",
    "    'optimizer':     InvalidOptimizer,\n",
    "  }\n",
    "\n",
    "  input:Iterable = []\n",
    "  output:float = 0.0\n",
    "\n",
    "  delta:float = 0.0\n",
    "\n",
    "\n",
    "  weight_gradient:Iterable = []\n",
    "  bias_gradient:float = 0.0\n",
    "\n",
    "\n",
    "\n",
    "  # \n",
    "  # Constructor\n",
    "  # \n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "\n",
    "      input_size:int,\n",
    "      \n",
    "      learning_rate:float=0.1,\n",
    "      activation:str|object='default',\n",
    "      loss_function:str|object='default',\n",
    "      optimizer:str|object='default',\n",
    "\n",
    "      initial_weights:Number|Iterable=0,\n",
    "      initial_bias:Number=0,\n",
    "    ):\n",
    "\n",
    "    #> input size and learning rate\n",
    "\n",
    "    # Input Size\n",
    "    self.input_size = input_size\n",
    "    self.input = np.repeat(0, self.input_size)\n",
    "    \n",
    "    # Learning Rate\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "    #> activation, loss_function and optimizer\n",
    "\n",
    "    params = {\n",
    "      'activation':    activation,\n",
    "      'loss_function': loss_function,\n",
    "      'optimizer':     optimizer,\n",
    "    }\n",
    "\n",
    "    # iterate all\n",
    "    for target in ['activation', 'loss_function', 'optimizer']:\n",
    "\n",
    "      # get param\n",
    "      param = params[target]\n",
    "      \n",
    "\n",
    "      # if present in builtin, set\n",
    "      if type(param) == str and param in self.builtin[target].keys():\n",
    "        setattr(self, target, self.builtin[target][param]())\n",
    "\n",
    "      # if object, set directly\n",
    "      elif isinstance(param, object): setattr(self, target, param)\n",
    "      \n",
    "      # raise error\n",
    "      # if not in {object, str} or string and not in builtin\n",
    "      else: raise self.errors[target](param)\n",
    "\n",
    "\n",
    "    #> Initial Weights & Bias\n",
    "\n",
    "    # initial weights\n",
    "    if isinstance(initial_weights, Number):                                            self.weights = np.repeat(initial_weights, input_size).astype(float)\n",
    "    elif isinstance(initial_weights, Iterable) and len(initial_weights) == input_size: self.weights = np.array(initial_weights).astype(float)\n",
    "    else: raise InvalidInitialWeights(f'Value must be either a number or an Iterable of size {input_size}')\n",
    "\n",
    "    # initialize gradient to zero\n",
    "    self.weight_gradient = np.repeat(0.0, self.input_size)\n",
    "\n",
    "    # initial bias\n",
    "    self.bias = initial_bias\n",
    "\n",
    "\n",
    "  \n",
    "  #\n",
    "  # Forward\n",
    "  #\n",
    "\n",
    "  def forward(self, input):\n",
    "    self.input = np.array(input)\n",
    "\n",
    "    # if 1d, predict\n",
    "    if len(self.input.shape) and self.input.shape[0] == self.input_size:\n",
    "      # weighted sum\n",
    "      weighted_sum = np.dot(self.input, self.weights) + self.bias\n",
    "\n",
    "      # activation\n",
    "      self.output = self.activation.apply(weighted_sum)\n",
    "\n",
    "      # return\n",
    "      return self.output\n",
    "    \n",
    "    # elif 2d, predict multiple\n",
    "    return self.forward_multiple(self.input)\n",
    "\n",
    "\n",
    "\n",
    "  #\n",
    "  # Forward Multiple samples\n",
    "  #\n",
    "\n",
    "  def forward_multiple(self, inputs):\n",
    "    inputs = np.array(inputs)\n",
    "\n",
    "    if len(inputs.shape) < 2 or inputs.shape[1] != self.input_size:\n",
    "      raise UnexpectedInputsShape(f'Inputs shape must be (n, {self.input_size}). found {inputs.shape}')\n",
    "\n",
    "    results = []\n",
    "    for inp in inputs:\n",
    "      results.append(self.forward(inp))\n",
    "\n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "\n",
    "  #\n",
    "  # Backward Pass (loss & optimize)\n",
    "  #\n",
    "\n",
    "  def backward(self, error=None):\n",
    "    self.error = error or self.error\n",
    "\n",
    "    # gradient\n",
    "    self.delta = self.error * self.activation.derivative(self.output)\n",
    "\n",
    "    # gradients for weights and bias\n",
    "    self.weight_gradient = self.delta * self.input\n",
    "    self.bias_gradient = self.delta\n",
    "\n",
    "    # return loss for previous neurons\n",
    "    return np.dot(self.delta, self.weights)\n",
    "\n",
    "\n",
    "  def update(self, learning_rate=None):\n",
    "    lr = learning_rate or self.learning_rate\n",
    "\n",
    "    self.weights -= self.weight_gradient * lr\n",
    "    self.bias    -= self.bias_gradient * lr\n",
    "\n",
    "\n",
    "  #\n",
    "  # Train\n",
    "  #\n",
    "\n",
    "  def train(self, inputs:Iterable, outputs:Iterable, epochs:int=100, batch_size:int=None, verbose:int=1):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    if len(inputs.shape) < 2 or inputs.shape[1] != self.input_size:    raise UnexpectedInputsShape (f'Inputs shape must be (n, {self.input_size}). found {inputs.shape}')\n",
    "    if len(outputs.shape) != 1 or outputs.shape[0] != inputs.shape[0]: raise UnexpectedOutputsShape(f'Outputs size ({outputs.shape[0]}) differs from no of samples in input ({inputs.shape[0]})')\n",
    "\n",
    "    # batch size: set counter\n",
    "    if batch_size:\n",
    "      self.batch_size = batch_size\n",
    "      self.batch_counter = 0\n",
    "\n",
    "    # batch reset\n",
    "    def reset_batch(): self.predicted = []\n",
    "\n",
    "    # batch end\n",
    "    def batch_end():\n",
    "      self.loss = self.loss_function.calculate(outputs, self.predicted)\n",
    "      if verbose: print(f'epoch={e+1}, loss={self.loss}, samples={len(self.predicted)}')\n",
    "\n",
    "    reset_batch()\n",
    "\n",
    "    # main loop\n",
    "    for e in range(epochs):\n",
    "      \n",
    "      if not batch_size: reset_batch()\n",
    "      \n",
    "      for i in range(outputs.shape[0]):\n",
    "\n",
    "        # Forward Pass\n",
    "        self.forward(inputs[i])\n",
    "        self.predicted.append(self.output)\n",
    "\n",
    "        # error calculation\n",
    "        self.error = self.output - outputs[i]\n",
    "\n",
    "        # Backward Pass\n",
    "        self.backward()\n",
    "\n",
    "        # Weights Updation\n",
    "        if self.optimizer.mode == self.optimizer.MODES['SAMPLE']:\n",
    "          self.optimizer.update(self)\n",
    "\n",
    "        if verbose > 1: print(f'epoch={e+1}, sample={i+1}, actual={outputs[i]}, predicted={self.output}, error={self.error}')\n",
    "      \n",
    "        # batch logic\n",
    "        if batch_size:\n",
    "          self.batch_counter += 1\n",
    "          if self.batch_counter >= self.batch_size:\n",
    "            batch_end()\n",
    "\n",
    "            if self.optimizer.mode == self.optimizer.MODES['BATCH']:\n",
    "              self.optimizer.update(self)\n",
    "\n",
    "            self.batch_counter = 0\n",
    "            reset_batch()\n",
    "\n",
    "      # epoch end (if batch_size=None)\n",
    "      if not batch_size: batch_end()\n",
    "    \n",
    "    if batch_size and len(self.predicted):\n",
    "      batch_end()\n",
    "\n",
    "\n",
    "\n",
    "  #\n",
    "  # Test\n",
    "  #\n",
    "\n",
    "  def test(self, inputs:Iterable, outputs:Iterable, verbose:int=0, binary:bool=True):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    if len(inputs.shape) < 2 or inputs.shape[1] != self.input_size:    raise UnexpectedInputsShape (f'Inputs shape must be (n, {self.input_size}). found {inputs.shape}')\n",
    "    if len(outputs.shape) != 1 or outputs.shape[0] != inputs.shape[0]: raise UnexpectedOutputsShape(f'Outputs size ({outputs.shape[0]}) differs from no of samples in input ({inputs.shape[0]})')\n",
    "    \n",
    "\n",
    "    accurate = 0\n",
    "    wrong    = 0\n",
    "    for i in range(inputs.shape[0]):\n",
    "\n",
    "      # Forward Pass\n",
    "      predicted = self.forward(inputs[i])\n",
    "      if binary: predicted = int(round(predicted))\n",
    "\n",
    "      if predicted == outputs[i]: accurate+=1\n",
    "      else:                       wrong   +=1\n",
    "\n",
    "      if verbose > 1: print(f'sample={i+1}, predicted={predicted}, actual={outputs[i]}, error={outputs[i] - predicted}')\n",
    "\n",
    "    return {\n",
    "      'total':    accurate + wrong,\n",
    "      'accurate': accurate,\n",
    "      'wrong':    wrong,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "from dataset import X, Y\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, loss=0.4432049847947397, samples=10\n",
      "epoch=2, loss=0.39826789813019803, samples=10\n",
      "epoch=3, loss=0.38800732978082764, samples=10\n",
      "epoch=4, loss=0.3782407475394864, samples=10\n",
      "epoch=5, loss=0.36902108828904506, samples=10\n",
      "epoch=6, loss=0.3603635683919453, samples=10\n",
      "epoch=7, loss=0.35224540908069013, samples=10\n",
      "epoch=8, loss=0.3446129230980055, samples=10\n",
      "epoch=9, loss=0.3373906798764574, samples=10\n",
      "epoch=10, loss=0.3304906628152556, samples=10\n",
      "epoch=11, loss=0.32382023163393525, samples=10\n",
      "epoch=12, loss=0.31728856326031585, samples=10\n",
      "epoch=13, loss=0.31081178367288426, samples=10\n",
      "epoch=14, loss=0.30431718570159794, samples=10\n",
      "epoch=15, loss=0.2977468389299578, samples=10\n",
      "epoch=16, loss=0.2910606577910456, samples=10\n",
      "epoch=17, loss=0.28423872288841334, samples=10\n",
      "epoch=18, loss=0.2772824550265156, samples=10\n",
      "epoch=19, loss=0.2702141990468712, samples=10\n",
      "epoch=20, loss=0.2630749157787413, samples=10\n",
      "epoch=21, loss=0.255919979438577, samples=10\n",
      "epoch=22, loss=0.2488134595671943, samples=10\n",
      "epoch=23, loss=0.24182162400530832, samples=10\n",
      "epoch=24, loss=0.23500661678779305, samples=10\n",
      "epoch=25, loss=0.2284212528583222, samples=10\n",
      "epoch=26, loss=0.2221056117184573, samples=10\n",
      "epoch=27, loss=0.2160856831543073, samples=10\n",
      "epoch=28, loss=0.21037387008335223, samples=10\n",
      "epoch=29, loss=0.20497083435388297, samples=10\n",
      "epoch=30, loss=0.19986805517642384, samples=10\n",
      "epoch=31, loss=0.195050536545601, samples=10\n",
      "epoch=32, loss=0.19049926985104118, samples=10\n",
      "epoch=33, loss=0.18619324563952888, samples=10\n",
      "epoch=34, loss=0.18211095896523913, samples=10\n",
      "epoch=35, loss=0.17823144720478687, samples=10\n",
      "epoch=36, loss=0.17453494361096572, samples=10\n",
      "epoch=37, loss=0.17100324001832154, samples=10\n",
      "epoch=38, loss=0.1676198435403256, samples=10\n",
      "epoch=39, loss=0.1643699960161528, samples=10\n",
      "epoch=40, loss=0.1612406079209737, samples=10\n",
      "epoch=41, loss=0.1582201435314286, samples=10\n",
      "epoch=42, loss=0.15529848232944285, samples=10\n",
      "epoch=43, loss=0.15246677286494065, samples=10\n",
      "epoch=44, loss=0.1497172890881086, samples=10\n",
      "epoch=45, loss=0.14704329491935897, samples=10\n",
      "epoch=46, loss=0.1444389200204846, samples=10\n",
      "epoch=47, loss=0.14189904793799885, samples=10\n",
      "epoch=48, loss=0.1394192166893022, samples=10\n",
      "epoch=49, loss=0.13699553122353414, samples=10\n",
      "epoch=50, loss=0.1346245868506502, samples=10\n",
      "epoch=51, loss=0.13230340258432038, samples=10\n",
      "epoch=52, loss=0.1300293633129474, samples=10\n",
      "epoch=53, loss=0.12780016974994943, samples=10\n",
      "epoch=54, loss=0.1256137951881175, samples=10\n",
      "epoch=55, loss=0.12346844817360807, samples=10\n",
      "epoch=56, loss=0.12136254031083679, samples=10\n",
      "epoch=57, loss=0.11929465850310998, samples=10\n",
      "epoch=58, loss=0.11726354102135575, samples=10\n",
      "epoch=59, loss=0.11526805687292252, samples=10\n",
      "epoch=60, loss=0.1133071880134566, samples=10\n",
      "epoch=61, loss=0.11138001400747052, samples=10\n",
      "epoch=62, loss=0.10948569879784194, samples=10\n",
      "epoch=63, loss=0.10762347929186569, samples=10\n",
      "epoch=64, loss=0.10579265551239378, samples=10\n",
      "epoch=65, loss=0.10399258209780533, samples=10\n",
      "epoch=66, loss=0.10222266096479773, samples=10\n",
      "epoch=67, loss=0.10048233497395054, samples=10\n",
      "epoch=68, loss=0.09877108246027724, samples=10\n",
      "epoch=69, loss=0.09708841251008257, samples=10\n",
      "epoch=70, loss=0.09543386088183858, samples=10\n",
      "epoch=71, loss=0.0938069864828715, samples=10\n",
      "epoch=72, loss=0.09220736832575754, samples=10\n",
      "epoch=73, loss=0.09063460289875093, samples=10\n",
      "epoch=74, loss=0.08908830189353711, samples=10\n",
      "epoch=75, loss=0.08756809024135823, samples=10\n",
      "epoch=76, loss=0.08607360441524303, samples=10\n",
      "epoch=77, loss=0.08460449096185833, samples=10\n",
      "epoch=78, loss=0.08316040523150378, samples=10\n",
      "epoch=79, loss=0.08174101027910553, samples=10\n",
      "epoch=80, loss=0.08034597591281914, samples=10\n",
      "epoch=81, loss=0.07897497787010452, samples=10\n",
      "epoch=82, loss=0.07762769710395759, samples=10\n",
      "epoch=83, loss=0.07630381916442235, samples=10\n",
      "epoch=84, loss=0.07500303366262333, samples=10\n",
      "epoch=85, loss=0.07372503380638751, samples=10\n",
      "epoch=86, loss=0.07246951599809956, samples=10\n",
      "epoch=87, loss=0.07123617948679939, samples=10\n",
      "epoch=88, loss=0.07002472606769897, samples=10\n",
      "epoch=89, loss=0.06883485982329884, samples=10\n",
      "epoch=90, loss=0.0676662869011439, samples=10\n",
      "epoch=91, loss=0.06651871532398948, samples=10\n",
      "epoch=92, loss=0.06539185482876808, samples=10\n",
      "epoch=93, loss=0.06428541673127298, samples=10\n",
      "epoch=94, loss=0.06319911381391295, samples=10\n",
      "epoch=95, loss=0.062132660234264205, samples=10\n",
      "epoch=96, loss=0.061085771452446036, samples=10\n",
      "epoch=97, loss=0.06005816417560776, samples=10\n",
      "epoch=98, loss=0.05904955631801071, samples=10\n",
      "epoch=99, loss=0.058059666975367616, samples=10\n",
      "epoch=100, loss=0.05708821641223143, samples=10\n"
     ]
    }
   ],
   "source": [
    "neuron.train(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 10, 'accurate': 10, 'wrong': 0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron.test(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron(\n",
    "  input_size=X.shape[1],\n",
    "  learning_rate=0.1,\n",
    "  activation=Sigmoid(),\n",
    "  loss_function=MeanSquaredError(),\n",
    "  optimizer=StochasticGradientDescent(),\n",
    "  initial_weights=[0.1, 0.2, 0.3],\n",
    "  initial_bias=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, loss=0.43045228586346795, samples=10\n",
      "epoch=2, loss=0.41836967798739455, samples=10\n",
      "epoch=3, loss=0.4076938752225794, samples=10\n",
      "epoch=4, loss=0.39745878576084726, samples=10\n",
      "epoch=5, loss=0.38775760804506554, samples=10\n",
      "epoch=6, loss=0.37864335775930424, samples=10\n",
      "epoch=7, loss=0.37012215425377787, samples=10\n",
      "epoch=8, loss=0.362158957738572, samples=10\n",
      "epoch=9, loss=0.35468761575761687, samples=10\n",
      "epoch=10, loss=0.3476219673920628, samples=10\n",
      "epoch=11, loss=0.34086576928325546, samples=10\n",
      "epoch=12, loss=0.33432053304942116, samples=10\n",
      "epoch=13, loss=0.327891347332267, samples=10\n",
      "epoch=14, loss=0.3214912442244972, samples=10\n",
      "epoch=15, loss=0.3150447436109822, samples=10\n",
      "epoch=16, loss=0.30849101789856626, samples=10\n",
      "epoch=17, loss=0.3017867982154678, samples=10\n",
      "epoch=18, loss=0.29490880594022206, samples=10\n",
      "epoch=19, loss=0.28785524431357945, samples=10\n",
      "epoch=20, loss=0.2806458091236319, samples=10\n",
      "epoch=21, loss=0.2733198146185044, samples=10\n",
      "epoch=22, loss=0.2659323521988818, samples=10\n",
      "epoch=23, loss=0.2585488123899574, samples=10\n",
      "epoch=24, loss=0.25123847951374734, samples=10\n",
      "epoch=25, loss=0.24406813487786105, samples=10\n",
      "epoch=26, loss=0.23709660241879274, samples=10\n",
      "epoch=27, loss=0.2303709354874952, samples=10\n",
      "epoch=28, loss=0.2239245505258934, samples=10\n",
      "epoch=29, loss=0.21777719459017392, samples=10\n",
      "epoch=30, loss=0.2119363193313249, samples=10\n",
      "epoch=31, loss=0.20639929524925496, samples=10\n",
      "epoch=32, loss=0.2011559287309829, samples=10\n",
      "epoch=33, loss=0.19619087792771697, samples=10\n",
      "epoch=34, loss=0.19148572812057746, samples=10\n",
      "epoch=35, loss=0.18702063000681293, samples=10\n",
      "epoch=36, loss=0.1827755025538363, samples=10\n",
      "epoch=37, loss=0.1787308558437471, samples=10\n",
      "epoch=38, loss=0.17486830976128223, samples=10\n",
      "epoch=39, loss=0.17117088442361428, samples=10\n",
      "epoch=40, loss=0.167623128270937, samples=10\n",
      "epoch=41, loss=0.1642111364379166, samples=10\n",
      "epoch=42, loss=0.16092249904747785, samples=10\n",
      "epoch=43, loss=0.157746207998846, samples=10\n",
      "epoch=44, loss=0.15467254208112527, samples=10\n",
      "epoch=45, loss=0.15169294368350877, samples=10\n",
      "epoch=46, loss=0.14879989562805473, samples=10\n",
      "epoch=47, loss=0.1459868033188995, samples=10\n",
      "epoch=48, loss=0.1432478851231937, samples=10\n",
      "epoch=49, loss=0.14057807238210346, samples=10\n",
      "epoch=50, loss=0.1379729194708797, samples=10\n",
      "epoch=51, loss=0.13542852371954012, samples=10\n",
      "epoch=52, loss=0.132941454650503, samples=10\n",
      "epoch=53, loss=0.1305086918019639, samples=10\n",
      "epoch=54, loss=0.12812757032699268, samples=10\n",
      "epoch=55, loss=0.1257957335478473, samples=10\n",
      "epoch=56, loss=0.1235110916755332, samples=10\n",
      "epoch=57, loss=0.12127178595804133, samples=10\n",
      "epoch=58, loss=0.11907615758514582, samples=10\n",
      "epoch=59, loss=0.11692272074563288, samples=10\n",
      "epoch=60, loss=0.11481013929978509, samples=10\n",
      "epoch=61, loss=0.11273720659320956, samples=10\n",
      "epoch=62, loss=0.11070282799629613, samples=10\n",
      "epoch=63, loss=0.1087060058061708, samples=10\n",
      "epoch=64, loss=0.10674582619487913, samples=10\n",
      "epoch=65, loss=0.10482144792895923, samples=10\n",
      "epoch=66, loss=0.1029320926219102, samples=10\n",
      "epoch=67, loss=0.10107703631281464, samples=10\n",
      "epoch=68, loss=0.09925560219201521, samples=10\n",
      "epoch=69, loss=0.09746715431875824, samples=10\n",
      "epoch=70, loss=0.0957110921965367, samples=10\n",
      "epoch=71, loss=0.09398684608991685, samples=10\n",
      "epoch=72, loss=0.09229387298225737, samples=10\n",
      "epoch=73, loss=0.09063165308727686, samples=10\n",
      "epoch=74, loss=0.0889996868391495, samples=10\n",
      "epoch=75, loss=0.08739749229597252, samples=10\n",
      "epoch=76, loss=0.08582460290026273, samples=10\n",
      "epoch=77, loss=0.08428056554777347, samples=10\n",
      "epoch=78, loss=0.08276493892255199, samples=10\n",
      "epoch=79, loss=0.0812772920619043, samples=10\n",
      "epoch=80, loss=0.07981720311991766, samples=10\n",
      "epoch=81, loss=0.07838425830251539, samples=10\n",
      "epoch=82, loss=0.07697805095076866, samples=10\n",
      "epoch=83, loss=0.0755981807524334, samples=10\n",
      "epoch=84, loss=0.07424425306449119, samples=10\n",
      "epoch=85, loss=0.07291587833190476, samples=10\n",
      "epoch=86, loss=0.07161267158989101, samples=10\n",
      "epoch=87, loss=0.07033425203882387, samples=10\n",
      "epoch=88, loss=0.06908024268243092, samples=10\n",
      "epoch=89, loss=0.06785027002127816, samples=10\n",
      "epoch=90, loss=0.0666439637946803, samples=10\n",
      "epoch=91, loss=0.06546095676514144, samples=10\n",
      "epoch=92, loss=0.06430088454025919, samples=10\n",
      "epoch=93, loss=0.0631633854277245, samples=10\n",
      "epoch=94, loss=0.06204810031964077, samples=10\n",
      "epoch=95, loss=0.06095467260288292, samples=10\n",
      "epoch=96, loss=0.05988274809263222, samples=10\n",
      "epoch=97, loss=0.058831974986574755, samples=10\n",
      "epoch=98, loss=0.057802003837537405, samples=10\n",
      "epoch=99, loss=0.05679248754257845, samples=10\n",
      "epoch=100, loss=0.055803081346752235, samples=10\n",
      "epoch=101, loss=0.05483344285992917, samples=10\n",
      "epoch=102, loss=0.053883232085195895, samples=10\n",
      "epoch=103, loss=0.05295211145747423, samples=10\n",
      "epoch=104, loss=0.05203974589109461, samples=10\n",
      "epoch=105, loss=0.05114580283514673, samples=10\n",
      "epoch=106, loss=0.050269952335500225, samples=10\n",
      "epoch=107, loss=0.0494118671024531, samples=10\n",
      "epoch=108, loss=0.048571222583024695, samples=10\n",
      "epoch=109, loss=0.04774769703696367, samples=10\n",
      "epoch=110, loss=0.04694097161559143, samples=10\n",
      "epoch=111, loss=0.04615073044265135, samples=10\n",
      "epoch=112, loss=0.04537666069638103, samples=10\n",
      "epoch=113, loss=0.0446184526920738, samples=10\n",
      "epoch=114, loss=0.04387579996443991, samples=10\n",
      "epoch=115, loss=0.043148399349129785, samples=10\n",
      "epoch=116, loss=0.04243595106282199, samples=10\n",
      "epoch=117, loss=0.04173815878133477, samples=10\n",
      "epoch=118, loss=0.041054729715259, samples=10\n",
      "epoch=119, loss=0.04038537468266449, samples=10\n",
      "epoch=120, loss=0.03972980817847377, samples=10\n",
      "epoch=121, loss=0.03908774844014706, samples=10\n",
      "epoch=122, loss=0.038458917509365226, samples=10\n",
      "epoch=123, loss=0.03784304128944458, samples=10\n",
      "epoch=124, loss=0.03723984959825748, samples=10\n",
      "epoch=125, loss=0.03664907621647702, samples=10\n",
      "epoch=126, loss=0.03607045893100241, samples=10\n",
      "epoch=127, loss=0.03550373957345966, samples=10\n",
      "epoch=128, loss=0.034948664053709264, samples=10\n",
      "epoch=129, loss=0.034404982388324255, samples=10\n",
      "epoch=130, loss=0.033872448724036094, samples=10\n",
      "epoch=131, loss=0.03335082135617092, samples=10\n",
      "epoch=132, loss=0.03283986274213114, samples=10\n",
      "epoch=133, loss=0.03233933950999512, samples=10\n",
      "epoch=134, loss=0.031849022462334856, samples=10\n",
      "epoch=135, loss=0.03136868657536802, samples=10\n",
      "epoch=136, loss=0.030898110993579152, samples=10\n",
      "epoch=137, loss=0.03043707901995995, samples=10\n",
      "epoch=138, loss=0.02998537810203033, samples=10\n",
      "epoch=139, loss=0.029542799813813712, samples=10\n",
      "epoch=140, loss=0.029109139833949305, samples=10\n",
      "epoch=141, loss=0.028684197920128617, samples=10\n",
      "epoch=142, loss=0.028267777880052407, samples=10\n",
      "epoch=143, loss=0.027859687539104495, samples=10\n",
      "epoch=144, loss=0.027459738704943188, samples=10\n",
      "epoch=145, loss=0.027067747129211183, samples=10\n",
      "epoch=146, loss=0.02668353246656448, samples=10\n",
      "epoch=147, loss=0.026306918231218935, samples=10\n",
      "epoch=148, loss=0.02593773175121165, samples=10\n",
      "epoch=149, loss=0.025575804120569822, samples=10\n",
      "epoch=150, loss=0.025220970149575982, samples=10\n",
      "epoch=151, loss=0.024873068313314622, samples=10\n",
      "epoch=152, loss=0.024531940698678017, samples=10\n",
      "epoch=153, loss=0.024197432950004448, samples=10\n",
      "epoch=154, loss=0.023869394213516568, samples=10\n",
      "epoch=155, loss=0.023547677080718067, samples=10\n",
      "epoch=156, loss=0.0232321375309042, samples=10\n",
      "epoch=157, loss=0.022922634872930732, samples=10\n",
      "epoch=158, loss=0.022619031686381803, samples=10\n",
      "epoch=159, loss=0.02232119376226812, samples=10\n",
      "epoch=160, loss=0.022028990043380797, samples=10\n",
      "epoch=161, loss=0.021742292564418807, samples=10\n",
      "epoch=162, loss=0.02146097639200053, samples=10\n",
      "epoch=163, loss=0.02118491956466357, samples=10\n",
      "epoch=164, loss=0.020914003032949543, samples=10\n",
      "epoch=165, loss=0.0206481105996651, samples=10\n",
      "epoch=166, loss=0.02038712886040194, samples=10\n",
      "epoch=167, loss=0.020130947144394406, samples=10\n",
      "epoch=168, loss=0.019879457455786252, samples=10\n",
      "epoch=169, loss=0.019632554415371834, samples=10\n",
      "epoch=170, loss=0.019390135202872614, samples=10\n",
      "epoch=171, loss=0.01915209949980318, samples=10\n",
      "epoch=172, loss=0.01891834943297739, samples=10\n",
      "epoch=173, loss=0.018688789518698356, samples=10\n",
      "epoch=174, loss=0.018463326607673667, samples=10\n",
      "epoch=175, loss=0.01824186983069096, samples=10\n",
      "epoch=176, loss=0.01802433054508556, samples=10\n",
      "epoch=177, loss=0.017810622282028378, samples=10\n",
      "epoch=178, loss=0.017600660694657804, samples=10\n",
      "epoch=179, loss=0.01739436350707637, samples=10\n",
      "epoch=180, loss=0.017191650464229397, samples=10\n",
      "epoch=181, loss=0.01699244328268025, samples=10\n",
      "epoch=182, loss=0.016796665602293502, samples=10\n",
      "epoch=183, loss=0.016604242938834834, samples=10\n",
      "epoch=184, loss=0.01641510263749458, samples=10\n",
      "epoch=185, loss=0.016229173827338336, samples=10\n",
      "epoch=186, loss=0.016046387376687433, samples=10\n",
      "epoch=187, loss=0.015866675849428892, samples=10\n",
      "epoch=188, loss=0.015689973462253926, samples=10\n",
      "epoch=189, loss=0.015516216042820807, samples=10\n",
      "epoch=190, loss=0.015345340988838154, samples=10\n",
      "epoch=191, loss=0.01517728722806248, samples=10\n",
      "epoch=192, loss=0.015011995179202157, samples=10\n",
      "epoch=193, loss=0.014849406713720276, samples=10\n",
      "epoch=194, loss=0.01468946511852606, samples=10\n",
      "epoch=195, loss=0.014532115059545395, samples=10\n",
      "epoch=196, loss=0.014377302546158799, samples=10\n",
      "epoch=197, loss=0.01422497489649539, samples=10\n",
      "epoch=198, loss=0.01407508070357028, samples=10\n",
      "epoch=199, loss=0.01392756980225257, samples=10\n",
      "epoch=200, loss=0.013782393237050387, samples=10\n",
      "epoch=201, loss=0.013639503230699377, samples=10\n",
      "epoch=202, loss=0.013498853153540338, samples=10\n",
      "epoch=203, loss=0.013360397493671674, samples=10\n",
      "epoch=204, loss=0.013224091827862022, samples=10\n",
      "epoch=205, loss=0.013089892793208043, samples=10\n",
      "epoch=206, loss=0.01295775805952325, samples=10\n",
      "epoch=207, loss=0.012827646302441351, samples=10\n",
      "epoch=208, loss=0.012699517177220846, samples=10\n",
      "epoch=209, loss=0.012573331293234378, samples=10\n",
      "epoch=210, loss=0.012449050189128587, samples=10\n",
      "epoch=211, loss=0.012326636308639163, samples=10\n",
      "epoch=212, loss=0.012206052977046411, samples=10\n",
      "epoch=213, loss=0.012087264378256172, samples=10\n",
      "epoch=214, loss=0.0119702355324918, samples=10\n",
      "epoch=215, loss=0.011854932274582472, samples=10\n",
      "epoch=216, loss=0.011741321232833423, samples=10\n",
      "epoch=217, loss=0.011629369808463914, samples=10\n",
      "epoch=218, loss=0.011519046155599053, samples=10\n",
      "epoch=219, loss=0.0114103191618015, samples=10\n",
      "epoch=220, loss=0.011303158429129373, samples=10\n",
      "epoch=221, loss=0.011197534255707471, samples=10\n",
      "epoch=222, loss=0.01109341761779787, samples=10\n",
      "epoch=223, loss=0.010990780152357566, samples=10\n",
      "epoch=224, loss=0.010889594140070487, samples=10\n",
      "epoch=225, loss=0.010789832488840847, samples=10\n",
      "epoch=226, loss=0.010691468717736498, samples=10\n",
      "epoch=227, loss=0.01059447694136953, samples=10\n",
      "epoch=228, loss=0.010498831854702984, samples=10\n",
      "epoch=229, loss=0.010404508718271935, samples=10\n",
      "epoch=230, loss=0.010311483343807975, samples=10\n",
      "epoch=231, loss=0.010219732080255863, samples=10\n",
      "epoch=232, loss=0.010129231800171836, samples=10\n",
      "epoch=233, loss=0.010039959886493395, samples=10\n",
      "epoch=234, loss=0.009951894219669622, samples=10\n",
      "epoch=235, loss=0.009865013165142968, samples=10\n",
      "epoch=236, loss=0.009779295561172281, samples=10\n",
      "epoch=237, loss=0.009694720706987504, samples=10\n",
      "epoch=238, loss=0.00961126835126738, samples=10\n",
      "epoch=239, loss=0.009528918680930512, samples=10\n",
      "epoch=240, loss=0.00944765231023153, samples=10\n",
      "epoch=241, loss=0.009367450270153546, samples=10\n",
      "epoch=242, loss=0.00928829399808858, samples=10\n",
      "epoch=243, loss=0.009210165327798106, samples=10\n",
      "epoch=244, loss=0.009133046479645511, samples=10\n",
      "epoch=245, loss=0.009056920051093025, samples=10\n",
      "epoch=246, loss=0.008981769007455627, samples=10\n",
      "epoch=247, loss=0.008907576672904545, samples=10\n",
      "epoch=248, loss=0.008834326721713475, samples=10\n",
      "epoch=249, loss=0.008762003169740493, samples=10\n",
      "epoch=250, loss=0.00869059036613907, samples=10\n",
      "epoch=251, loss=0.008620072985291617, samples=10\n",
      "epoch=252, loss=0.00855043601895937, samples=10\n",
      "epoch=253, loss=0.00848166476864248, samples=10\n",
      "epoch=254, loss=0.008413744838144074, samples=10\n",
      "epoch=255, loss=0.008346662126332933, samples=10\n",
      "epoch=256, loss=0.00828040282009884, samples=10\n",
      "epoch=257, loss=0.008214953387495144, samples=10\n",
      "epoch=258, loss=0.008150300571063451, samples=10\n",
      "epoch=259, loss=0.008086431381335078, samples=10\n",
      "epoch=260, loss=0.008023333090504303, samples=10\n",
      "epoch=261, loss=0.00796099322626856, samples=10\n",
      "epoch=262, loss=0.007899399565830908, samples=10\n",
      "epoch=263, loss=0.00783854013006006, samples=10\n",
      "epoch=264, loss=0.007778403177803694, samples=10\n",
      "epoch=265, loss=0.007718977200350452, samples=10\n",
      "epoch=266, loss=0.007660250916036754, samples=10\n",
      "epoch=267, loss=0.007602213264994047, samples=10\n",
      "epoch=268, loss=0.007544853404032705, samples=10\n",
      "epoch=269, loss=0.007488160701658744, samples=10\n",
      "epoch=270, loss=0.007432124733219504, samples=10\n",
      "epoch=271, loss=0.007376735276174736, samples=10\n",
      "epoch=272, loss=0.007321982305489559, samples=10\n",
      "epoch=273, loss=0.007267855989146005, samples=10\n",
      "epoch=274, loss=0.007214346683769536, samples=10\n",
      "epoch=275, loss=0.007161444930367672, samples=10\n",
      "epoch=276, loss=0.007109141450177337, samples=10\n",
      "epoch=277, loss=0.0070574271406180136, samples=10\n",
      "epoch=278, loss=0.007006293071347742, samples=10\n",
      "epoch=279, loss=0.006955730480419137, samples=10\n",
      "epoch=280, loss=0.006905730770532513, samples=10\n",
      "epoch=281, loss=0.0068562855053836535, samples=10\n",
      "epoch=282, loss=0.0068073864061034385, samples=10\n",
      "epoch=283, loss=0.0067590253477867995, samples=10\n",
      "epoch=284, loss=0.006711194356108606, samples=10\n",
      "epoch=285, loss=0.006663885604024153, samples=10\n",
      "epoch=286, loss=0.006617091408551728, samples=10\n",
      "epoch=287, loss=0.006570804227635145, samples=10\n",
      "epoch=288, loss=0.0065250166570841334, samples=10\n",
      "epoch=289, loss=0.006479721427590211, samples=10\n",
      "epoch=290, loss=0.006434911401816299, samples=10\n",
      "epoch=291, loss=0.006390579571557693, samples=10\n",
      "epoch=292, loss=0.006346719054972962, samples=10\n",
      "epoch=293, loss=0.006303323093882242, samples=10\n",
      "epoch=294, loss=0.006260385051131756, samples=10\n",
      "epoch=295, loss=0.006217898408022277, samples=10\n",
      "epoch=296, loss=0.006175856761800054, samples=10\n",
      "epoch=297, loss=0.006134253823208499, samples=10\n",
      "epoch=298, loss=0.006093083414098833, samples=10\n",
      "epoch=299, loss=0.006052339465098545, samples=10\n",
      "epoch=300, loss=0.006012016013335482, samples=10\n",
      "epoch=301, loss=0.005972107200216706, samples=10\n",
      "epoch=302, loss=0.005932607269260256, samples=10\n",
      "epoch=303, loss=0.0058935105639785455, samples=10\n",
      "epoch=304, loss=0.005854811525812129, samples=10\n",
      "epoch=305, loss=0.0058165046921123025, samples=10\n",
      "epoch=306, loss=0.005778584694171427, samples=10\n",
      "epoch=307, loss=0.005741046255299602, samples=10\n",
      "epoch=308, loss=0.005703884188946547, samples=10\n",
      "epoch=309, loss=0.005667093396867528, samples=10\n",
      "epoch=310, loss=0.005630668867331994, samples=10\n",
      "epoch=311, loss=0.005594605673374159, samples=10\n",
      "epoch=312, loss=0.005558898971083989, samples=10\n",
      "epoch=313, loss=0.005523543997938037, samples=10\n",
      "epoch=314, loss=0.005488536071168645, samples=10\n",
      "epoch=315, loss=0.0054538705861708146, samples=10\n",
      "epoch=316, loss=0.0054195430149457016, samples=10\n",
      "epoch=317, loss=0.005385548904579738, samples=10\n",
      "epoch=318, loss=0.005351883875758502, samples=10\n",
      "epoch=319, loss=0.00531854362131449, samples=10\n",
      "epoch=320, loss=0.005285523904807897, samples=10\n",
      "epoch=321, loss=0.005252820559139481, samples=10\n",
      "epoch=322, loss=0.0052204294851948664, samples=10\n",
      "epoch=323, loss=0.005188346650519397, samples=10\n",
      "epoch=324, loss=0.005156568088022597, samples=10\n",
      "epoch=325, loss=0.005125089894711993, samples=10\n",
      "epoch=326, loss=0.005093908230454905, samples=10\n",
      "epoch=327, loss=0.005063019316767994, samples=10\n",
      "epoch=328, loss=0.005032419435633728, samples=10\n",
      "epoch=329, loss=0.005002104928342993, samples=10\n",
      "epoch=330, loss=0.004972072194363348, samples=10\n",
      "epoch=331, loss=0.004942317690232163, samples=10\n",
      "epoch=332, loss=0.004912837928474173, samples=10\n",
      "epoch=333, loss=0.004883629476542658, samples=10\n",
      "epoch=334, loss=0.0048546889557837805, samples=10\n",
      "epoch=335, loss=0.004826013040423587, samples=10\n",
      "epoch=336, loss=0.004797598456576909, samples=10\n",
      "epoch=337, loss=0.004769441981277795, samples=10\n",
      "epoch=338, loss=0.004741540441530911, samples=10\n",
      "epoch=339, loss=0.004713890713383342, samples=10\n",
      "epoch=340, loss=0.00468648972101639, samples=10\n",
      "epoch=341, loss=0.004659334435856822, samples=10\n",
      "epoch=342, loss=0.004632421875707054, samples=10\n",
      "epoch=343, loss=0.004605749103894035, samples=10\n",
      "epoch=344, loss=0.004579313228435984, samples=10\n",
      "epoch=345, loss=0.004553111401227042, samples=10\n",
      "epoch=346, loss=0.004527140817238921, samples=10\n",
      "epoch=347, loss=0.004501398713739627, samples=10\n",
      "epoch=348, loss=0.004475882369528347, samples=10\n",
      "epoch=349, loss=0.004450589104186567, samples=10\n",
      "epoch=350, loss=0.004425516277344827, samples=10\n",
      "epoch=351, loss=0.004400661287964601, samples=10\n",
      "epoch=352, loss=0.004376021573635326, samples=10\n",
      "epoch=353, loss=0.004351594609885849, samples=10\n",
      "epoch=354, loss=0.004327377909510193, samples=10\n",
      "epoch=355, loss=0.004303369021907169, samples=10\n",
      "epoch=356, loss=0.004279565532433611, samples=10\n",
      "epoch=357, loss=0.004255965061770844, samples=10\n",
      "epoch=358, loss=0.004232565265304122, samples=10\n",
      "epoch=359, loss=0.004209363832514757, samples=10\n",
      "epoch=360, loss=0.0041863584863844885, samples=10\n",
      "epoch=361, loss=0.004163546982812107, samples=10\n",
      "epoch=362, loss=0.004140927110041731, samples=10\n",
      "epoch=363, loss=0.004118496688102704, samples=10\n",
      "epoch=364, loss=0.004096253568260732, samples=10\n",
      "epoch=365, loss=0.004074195632479998, samples=10\n",
      "epoch=366, loss=0.004052320792896127, samples=10\n",
      "epoch=367, loss=0.004030626991299567, samples=10\n",
      "epoch=368, loss=0.004009112198629313, samples=10\n",
      "epoch=369, loss=0.003987774414476638, samples=10\n",
      "epoch=370, loss=0.003966611666598759, samples=10\n",
      "epoch=371, loss=0.003945622010441885, samples=10\n",
      "epoch=372, loss=0.003924803528673895, samples=10\n",
      "epoch=373, loss=0.0039041543307260277, samples=10\n",
      "epoch=374, loss=0.0038836725523436052, samples=10\n",
      "epoch=375, loss=0.003863356355145525, samples=10\n",
      "epoch=376, loss=0.0038432039261923545, samples=10\n",
      "epoch=377, loss=0.003823213477562759, samples=10\n",
      "epoch=378, loss=0.0038033832459381904, samples=10\n",
      "epoch=379, loss=0.003783711492195555, samples=10\n",
      "epoch=380, loss=0.003764196501007795, samples=10\n",
      "epoch=381, loss=0.0037448365804520455, samples=10\n",
      "epoch=382, loss=0.0037256300616254076, samples=10\n",
      "epoch=383, loss=0.0037065752982680202, samples=10\n",
      "epoch=384, loss=0.0036876706663932788, samples=10\n",
      "epoch=385, loss=0.0036689145639251784, samples=10\n",
      "epoch=386, loss=0.00365030541034242, samples=10\n",
      "epoch=387, loss=0.00363184164632938, samples=10\n",
      "epoch=388, loss=0.003613521733433561, samples=10\n",
      "epoch=389, loss=0.003595344153729488, samples=10\n",
      "epoch=390, loss=0.0035773074094890348, samples=10\n",
      "epoch=391, loss=0.003559410022857752, samples=10\n",
      "epoch=392, loss=0.003541650535537367, samples=10\n",
      "epoch=393, loss=0.0035240275084741377, samples=10\n",
      "epoch=394, loss=0.0035065395215529843, samples=10\n",
      "epoch=395, loss=0.0034891851732973605, samples=10\n",
      "epoch=396, loss=0.003471963080574579, samples=10\n",
      "epoch=397, loss=0.003454871878306609, samples=10\n",
      "epoch=398, loss=0.0034379102191862097, samples=10\n",
      "epoch=399, loss=0.0034210767733982178, samples=10\n",
      "epoch=400, loss=0.0034043702283460034, samples=10\n",
      "epoch=401, loss=0.0033877892883828293, samples=10\n",
      "epoch=402, loss=0.0033713326745481707, samples=10\n",
      "epoch=403, loss=0.0033549991243087595, samples=10\n",
      "epoch=404, loss=0.003338787391304399, samples=10\n",
      "epoch=405, loss=0.0033226962450982633, samples=10\n",
      "epoch=406, loss=0.003306724470931763, samples=10\n",
      "epoch=407, loss=0.003290870869483803, samples=10\n",
      "epoch=408, loss=0.003275134256634353, samples=10\n",
      "epoch=409, loss=0.0032595134632322346, samples=10\n",
      "epoch=410, loss=0.003244007334867008, samples=10\n",
      "epoch=411, loss=0.003228614731645054, samples=10\n",
      "epoch=412, loss=0.003213334527969408, samples=10\n",
      "epoch=413, loss=0.003198165612323732, samples=10\n",
      "epoch=414, loss=0.003183106887059909, samples=10\n",
      "epoch=415, loss=0.003168157268189449, samples=10\n",
      "epoch=416, loss=0.0031533156851785782, samples=10\n",
      "epoch=417, loss=0.003138581080746894, samples=10\n",
      "epoch=418, loss=0.0031239524106695415, samples=10\n",
      "epoch=419, loss=0.0031094286435828073, samples=10\n",
      "epoch=420, loss=0.003095008760793152, samples=10\n",
      "epoch=421, loss=0.003080691756089513, samples=10\n",
      "epoch=422, loss=0.0030664766355588615, samples=10\n",
      "epoch=423, loss=0.0030523624174049564, samples=10\n",
      "epoch=424, loss=0.0030383481317701965, samples=10\n",
      "epoch=425, loss=0.003024432820560555, samples=10\n",
      "epoch=426, loss=0.003010615537273488, samples=10\n",
      "epoch=427, loss=0.0029968953468288086, samples=10\n",
      "epoch=428, loss=0.002983271325402424, samples=10\n",
      "epoch=429, loss=0.002969742560262914, samples=10\n",
      "epoch=430, loss=0.0029563081496108826, samples=10\n",
      "epoch=431, loss=0.0029429672024210004, samples=10\n",
      "epoch=432, loss=0.0029297188382867434, samples=10\n",
      "epoch=433, loss=0.00291656218726775, samples=10\n",
      "epoch=434, loss=0.002903496389739735, samples=10\n",
      "epoch=435, loss=0.002890520596246862, samples=10\n",
      "epoch=436, loss=0.002877633967356705, samples=10\n",
      "epoch=437, loss=0.0028648356735175165, samples=10\n",
      "epoch=438, loss=0.0028521248949179307, samples=10\n",
      "epoch=439, loss=0.0028395008213489606, samples=10\n",
      "epoch=440, loss=0.00282696265206832, samples=10\n",
      "epoch=441, loss=0.0028145095956669526, samples=10\n",
      "epoch=442, loss=0.0028021408699377822, samples=10\n",
      "epoch=443, loss=0.002789855701746597, samples=10\n",
      "epoch=444, loss=0.002777653326905073, samples=10\n",
      "epoch=445, loss=0.0027655329900458684, samples=10\n",
      "epoch=446, loss=0.002753493944499787, samples=10\n",
      "epoch=447, loss=0.0027415354521748373, samples=10\n",
      "epoch=448, loss=0.0027296567834374165, samples=10\n",
      "epoch=449, loss=0.002717857216995285, samples=10\n",
      "epoch=450, loss=0.002706136039782491, samples=10\n",
      "epoch=451, loss=0.002694492546846168, samples=10\n",
      "epoch=452, loss=0.0026829260412351064, samples=10\n",
      "epoch=453, loss=0.0026714358338901377, samples=10\n",
      "epoch=454, loss=0.002660021243536304, samples=10\n",
      "epoch=455, loss=0.002648681596576698, samples=10\n",
      "epoch=456, loss=0.0026374162269880265, samples=10\n",
      "epoch=457, loss=0.0026262244762178485, samples=10\n",
      "epoch=458, loss=0.0026151056930834022, samples=10\n",
      "epoch=459, loss=0.0026040592336720736, samples=10\n",
      "epoch=460, loss=0.0025930844612434218, samples=10\n",
      "epoch=461, loss=0.0025821807461327605, samples=10\n",
      "epoch=462, loss=0.002571347465656209, samples=10\n",
      "epoch=463, loss=0.002560584004017316, samples=10\n",
      "epoch=464, loss=0.002549889752215078, samples=10\n",
      "epoch=465, loss=0.002539264107953417, samples=10\n",
      "epoch=466, loss=0.002528706475552049, samples=10\n",
      "epoch=467, loss=0.0025182162658588135, samples=10\n",
      "epoch=468, loss=0.002507792896163247, samples=10\n",
      "epoch=469, loss=0.0024974357901115774, samples=10\n",
      "epoch=470, loss=0.002487144377623036, samples=10\n",
      "epoch=471, loss=0.0024769180948073827, samples=10\n",
      "epoch=472, loss=0.002466756383883787, samples=10\n",
      "epoch=473, loss=0.0024566586931008974, samples=10\n",
      "epoch=474, loss=0.002446624476658158, samples=10\n",
      "epoch=475, loss=0.0024366531946283256, samples=10\n",
      "epoch=476, loss=0.002426744312881159, samples=10\n",
      "epoch=477, loss=0.002416897303008266, samples=10\n",
      "epoch=478, loss=0.002407111642249101, samples=10\n",
      "epoch=479, loss=0.0023973868134181038, samples=10\n",
      "epoch=480, loss=0.0023877223048328544, samples=10\n",
      "epoch=481, loss=0.002378117610243455, samples=10\n",
      "epoch=482, loss=0.0023685722287628075, samples=10\n",
      "epoch=483, loss=0.002359085664798077, samples=10\n",
      "epoch=484, loss=0.0023496574279831036, samples=10\n",
      "epoch=485, loss=0.002340287033111831, samples=10\n",
      "epoch=486, loss=0.0023309740000727595, samples=10\n",
      "epoch=487, loss=0.0023217178537843623, samples=10\n",
      "epoch=488, loss=0.0023125181241314295, samples=10\n",
      "epoch=489, loss=0.00230337434590239, samples=10\n",
      "epoch=490, loss=0.002294286058727552, samples=10\n",
      "epoch=491, loss=0.002285252807018245, samples=10\n",
      "epoch=492, loss=0.0022762741399068656, samples=10\n",
      "epoch=493, loss=0.0022673496111878183, samples=10\n",
      "epoch=494, loss=0.002258478779259254, samples=10\n",
      "epoch=495, loss=0.002249661207065768, samples=10\n",
      "epoch=496, loss=0.0022408964620418553, samples=10\n",
      "epoch=497, loss=0.0022321841160561733, samples=10\n",
      "epoch=498, loss=0.002223523745356733, samples=10\n",
      "epoch=499, loss=0.002214914930516711, samples=10\n",
      "epoch=500, loss=0.002206357256381207, samples=10\n"
     ]
    }
   ],
   "source": [
    "neuron.train(\n",
    "  inputs=X,\n",
    "  outputs=Y,\n",
    "  epochs=500,\n",
    "  verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.60686364, -0.23533579,  5.46429157])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.010045078534286"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron.bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
